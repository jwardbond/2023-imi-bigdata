{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import coo_array\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5fa4f",
   "metadata": {},
   "source": [
    "# 1. Zhang et al. Method\n",
    "This method uses the pagerank technique outlined in [Zhang *et al.,* 2022](https://arxiv.org/abs/2104.02764). \n",
    "\n",
    "Mainly: \n",
    "\n",
    "Let $M$ be a transition matrix with: \n",
    "$$m_{ij} = \\begin{cases}\n",
    "    \\theta w_{ji}/s_j^{out} + (1-\\theta)a_{ji}/d_j^{out} && \\texttt{if}\\; d_j^{out}\\neq 0\\\\\n",
    "    0 && \\texttt{if}\\; d_j^{out}=0\n",
    "\\end{cases}$$\n",
    "Where: \n",
    "- $\\theta$ is a tunable parameter that represents how important edge weights should be in the pagerank alg\n",
    "- $w_{ji}$ is the weight of an edge from node $j \\rightarrow i$\n",
    "- $s_j^{out} = \\sum_{v \\in V|j \\rightarrow v}w_{jv}$ is the \"strength\" of outgoing edges from node $j$\n",
    "- $a_{ji}$ = $1 \\:\\texttt{if}\\: j \\rightarrow i \\:\\texttt{else}\\: 0$\n",
    "- $d_j^{out} = \\sum_{v \\in V|j \\rightarrow v}a_{jv}$\n",
    "- $\\beta_i$ is the node importance score\n",
    "  \n",
    "\n",
    "\n",
    "Then the pagerank can be calculated with power iterations on\n",
    "$$P=\\gamma MP + (1-\\gamma)\\boldsymbol{\\beta}/||\\boldsymbol{\\beta}||_1$$\n",
    "\n",
    "Where:\n",
    "- $1-\\gamma$ is a tunable parameter representing the probability of restarting a random walk (typically 0.8-0.9)\n",
    "- $\\boldsymbol{\\beta}/||\\boldsymbol{\\beta}||_1$ is the vectorized version of $\\beta_{i}/\\sum_{i\\in V}\\beta_i$\n",
    "\n",
    "---\n",
    "*note the other formulation in the paper results in a dense transition matrix that is nxn.... too big*\n",
    "\n",
    "*note that for $m_{ij}$, $i$ is the target and $j$ is the source*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA = 1\n",
    "GAMMA = 0.85\n",
    "REVERSE = True\n",
    "\n",
    "DATAPATH = Path('../data/processed/pagerank')\n",
    "OUTPATH = DATAPATH.parent / 'nodes_new_rev.parquet'\n",
    "\n",
    "n_df = pd.read_parquet(DATAPATH / 'input_node_scores.parquet')\n",
    "e_df = pd.read_parquet(DATAPATH / 'input_edge_scores.parquet')\n",
    "e_df = e_df[['cust_id_sender', 'cust_id_receiver', 'score']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reversing edges\n",
    "if REVERSE: \n",
    "    e2_df = pd.DataFrame()\n",
    "    e2_df['cust_id_receiver'] = e_df['cust_id_sender'].copy()\n",
    "    e2_df['cust_id_sender'] = e_df['cust_id_receiver'].copy()\n",
    "    e2_df['score'] = e_df['score']\n",
    "\n",
    "    e_df = pd.concat([e_df, e2_df])\n",
    "    e_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282373f7",
   "metadata": {},
   "source": [
    "## Constructing $\\boldsymbol{\\beta}/||\\boldsymbol{\\beta}||_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all nodes in the graph\n",
    "node_list = []\n",
    "node_list.extend(e_df.cust_id_sender.tolist() + e_df.cust_id_receiver.tolist() + n_df.cust_id.tolist())\n",
    "node_list = list(set(node_list)) #hack to remove duplicate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct B\n",
    "b_df = pd.DataFrame(data={'cust_id':node_list})\n",
    "b_df = b_df.merge(n_df, on='cust_id', how='left')\n",
    "b_df = b_df.rename(columns={'score':'b_i'})\n",
    "b_df = b_df.fillna(0)\n",
    "\n",
    "one_norm = abs(b_df['b_i']).sum()\n",
    "\n",
    "b_df['b_i'] = b_df['b_i'] / one_norm\n",
    "\n",
    "b_df.sort_values('b_i', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568cd38",
   "metadata": {},
   "source": [
    "## Node Encoding\n",
    "This section encodes the cust_ids ordinally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(node_list)\n",
    "\n",
    "e_df['cust_id_sender'] = le.transform(e_df['cust_id_sender'])\n",
    "e_df['cust_id_receiver'] = le.transform(e_df['cust_id_receiver'])\n",
    "\n",
    "b_df['cust_id'] = le.transform(b_df['cust_id'])\n",
    "\n",
    "node_enc = le.transform(node_list)\n",
    "\n",
    "e_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28edfb4",
   "metadata": {},
   "source": [
    "## Constructing $M$\n",
    "*note that for $m_{ij}$, $i$ is the target node and $j$ is the source node*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df['score'] += 0.01 #No edges can have 0 weight, so we add a relatively small value to all edges\n",
    "\n",
    "#Calculate w_ji\n",
    "w_df = e_df.groupby(['cust_id_sender', 'cust_id_receiver'], as_index=False)['score'].sum()\n",
    "w_df = w_df.rename(columns={'score':'w_ji'})\n",
    "e_df = e_df.merge(w_df, on=['cust_id_sender', 'cust_id_receiver'], how='left')\n",
    "\n",
    "#Calculate a_ji\n",
    "a_df = e_df.groupby(['cust_id_sender', 'cust_id_receiver'], as_index=False)['score'].count()\n",
    "# a_df = e_df.groupby \n",
    "a_df = a_df.rename(columns={'score':'a_ji'})\n",
    "e_df = e_df.merge(a_df, on=['cust_id_sender', 'cust_id_receiver'], how='left')\n",
    "\n",
    "#Calculate s_j\n",
    "s_df = e_df.groupby(['cust_id_sender'], as_index=False)['score'].sum()\n",
    "s_df = s_df.rename(columns={'score':'s_j'})\n",
    "e_df = e_df.merge(s_df, on='cust_id_sender', how='left')\n",
    "\n",
    "#Calculate d_j\n",
    "d_df = e_df.groupby(['cust_id_sender'], as_index=False)['score'].count()\n",
    "d_df = d_df.rename(columns={'score':'d_j'})\n",
    "e_df = e_df.merge(d_df, on='cust_id_sender', how='left')\n",
    "\n",
    "#Remove duplicate edges... taking max is just a hack\n",
    "e_df = e_df.groupby(['cust_id_sender', 'cust_id_receiver'], as_index=False).max()\n",
    "\n",
    "e_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad839bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_m(r): \n",
    "    m = THETA*r.w_ji/r.s_j + (1-THETA)*r.a_ji/r.d_j\n",
    "    return m\n",
    "\n",
    "#calculate m_ij for sender j, receiver i\n",
    "e_df['m'] = e_df.apply(lambda r: calc_m(r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify matrix is column stochastic... output m should be 1\n",
    "e_df.groupby('cust_id_sender').sum().sort_values('m', ascending=True).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1646300",
   "metadata": {},
   "source": [
    "## Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81134ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pagerank(M, B, P, gamma=0.85, tol=1e-10, maxit=1000):\n",
    "    \"\"\"Computes the node and edge weighted pagerank\n",
    "    \n",
    "    This function takes in a transition matrix, node weights, and an initial pagerank vector\n",
    "    and computes the weighted pagerank, stopping when the difference between iterations is < \n",
    "    tolerence or when the maximum # iterations is reached.\n",
    "    \n",
    "    Args:\n",
    "        M: The transition matrix according to Zhang et al. should be a scipy sparse coordinate matrix (NxN)\n",
    "        B: A numpy array of relative node weights (Nx1)\n",
    "        P: A numpy array vector of initial pageranks, normally just uniform (Nx1) \n",
    "        gamma: A hyperparam representing the probability of a random surfer NOT making a random jump to a new node\n",
    "        tol: iteration tolerance - when the maximum RELATIVE change in a node score is below this value, iterations stop\n",
    "        maxit: maximum iterations\n",
    "    \n",
    "    Returns\n",
    "        P: The final pagerank vector, min-max scaled to be in [0-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    itcount = 0\n",
    "    max_diff = 1000 #placeholder large value\n",
    "    diff_list = []\n",
    "    while (itcount <= maxit) and (max_diff >= tol):\n",
    "           \n",
    "        #Pagerank iter\n",
    "        P_int = GAMMA*M.dot(P) + (1-GAMMA)*B\n",
    "                \n",
    "        #Adding leaked pagerank back\n",
    "        #need to do this since we don't preprocess to remove dead ends.\n",
    "        leak = np.sum(P_int)\n",
    "        P_int = P_int + (1-leak)/len(P)\n",
    "        \n",
    "        max_diff = (np.absolute(P-P_int)/P).max()\n",
    "        diff_list.append(max_diff)\n",
    "        itcount += 1\n",
    "        P = P_int\n",
    "        \n",
    "    \n",
    "    print(f'Final max (relative) error: {max_diff:.3}')\n",
    "    print(f'Final iterations: {itcount}')\n",
    "    # Scaling\n",
    "    P = (P - P.min())/(P.max() - P.min())\n",
    "    \n",
    "    return P, diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct necessary matrices\n",
    "i = e_df['cust_id_receiver'].values\n",
    "j = e_df['cust_id_sender'].values\n",
    "m = e_df['m'].values\n",
    "\n",
    "N = b_df.shape[0]\n",
    "B = b_df.sort_values('cust_id')['b_i'].values\n",
    "M = coo_array((m,(i,j)), shape=(N,N))\n",
    "P = np.full(N, 1/N)\n",
    "\n",
    "#Run Pagerank\n",
    "P, dl = weighted_pagerank(M, B, P, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b5eb3",
   "metadata": {},
   "source": [
    "## Exporting for Webapp\n",
    "The pagerank data needs to be converted back into KYC-esque data, with customer names, countries, etc. but this needs to include *external customers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pagerank_results(pagerank_df):\n",
    "    \"\"\"Takes in a [id, score] dataframe and turns it back into KYC-style data\"\"\"\n",
    "    \n",
    "    datapath = Path('../data/processed/')\n",
    "    \n",
    "    kyc_df = pd.read_parquet(datapath / 'kyc.parquet')\n",
    "    wdf = pd.read_parquet(datapath / 'wire.parquet')\n",
    "    edf = pd.read_parquet(datapath / 'emt.parquet')\n",
    "    \n",
    "    cleaned = pagerank_df.copy()\n",
    "    \n",
    "    # Join with kyc data and add country column\n",
    "    kyc_df['country'] = 'CA'\n",
    "    cleaned = pd.merge(cleaned, kyc_df, how='left', on='cust_id')\n",
    "\n",
    "    # Get additional countries from wiretransfer data\n",
    "    s1 = wdf[['cust_id_sender', 'country_sender']].copy().rename(columns={'cust_id_sender':'cust_id', 'country_sender':'country'})\n",
    "    s2 = wdf[['cust_id_receiver', 'country_receiver']].copy().rename(columns={'cust_id_receiver':'cust_id', 'country_receiver':'country'})\n",
    "    countries = pd.concat([s1,s2])\n",
    "    countries = countries.drop_duplicates()\n",
    "\n",
    "    cleaned = pd.merge(cleaned, countries, how='left', on='cust_id')\n",
    "    cleaned['country_x'] = cleaned['country_x'].combine_first(cleaned['country_y'])\n",
    "    cleaned['country'] = cleaned['country_x']\n",
    "    cleaned = cleaned.drop(columns=['country_x', 'country_y'])\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_df = pd.DataFrame(data=P, columns=['score'])\n",
    "P_df['cust_id'] = P_df.index\n",
    "P_df['cust_id'] = le.inverse_transform(P_df['cust_id']) #transform back to actual IDS\n",
    "P_df = P_df[['cust_id','score']]\n",
    "P_df = clean_pagerank_results(P_df)\n",
    "P_df.to_parquet(OUTPATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imibigdata]",
   "language": "python",
   "name": "conda-env-imibigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
